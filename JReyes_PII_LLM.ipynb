{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEVQyq+VSJuaT9L6BSck+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenniferreyesdev/jenniferreyesdev/blob/main/JReyes_PII_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prerequisites**\n",
        "\n",
        "Install from PyPI:"
      ],
      "metadata": {
        "id": "_Z6YwUFWldrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install pdfminer.six\n",
        "!pip install pikepdf\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sdv\n",
        "!pip install datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install torch==1.13.1\n"
      ],
      "metadata": {
        "id": "6FjyYqnGlfJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e7e2ea-c9e9-4bc3-eee5-7ffb0314e9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: presidio_analyzer in /usr/local/lib/python3.10/dist-packages (2.2.33)\n",
            "Requirement already satisfied: spacy>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from presidio_analyzer) (3.5.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from presidio_analyzer) (2022.10.31)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from presidio_analyzer) (3.4.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from presidio_analyzer) (6.0.1)\n",
            "Requirement already satisfied: phonenumbers>=8.12 in /usr/local/lib/python3.10/dist-packages (from presidio_analyzer) (8.13.18)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio_analyzer) (3.3.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio_analyzer) (3.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio_analyzer) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio_analyzer) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.4.4->presidio_analyzer) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio_analyzer) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract->presidio_analyzer) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio_analyzer) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio_analyzer) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.4.4->presidio_analyzer) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.4.4->presidio_analyzer) (2.1.3)\n",
            "Requirement already satisfied: presidio_anonymizer in /usr/local/lib/python3.10/dist-packages (2.2.33)\n",
            "Requirement already satisfied: pycryptodome>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from presidio_anonymizer) (3.18.0)\n",
            "2023-08-10 06:16:06.401006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20221105)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (8.2.3)\n",
            "Requirement already satisfied: Pillow>=9.0 in /usr/local/lib/python3.10/dist-packages (from pikepdf) (9.4.0)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.10/dist-packages (from pikepdf) (2.1.0)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.10/dist-packages (from pikepdf) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pikepdf) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: sdv in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: Faker<15,>=10 in /usr/local/lib/python3.10/dist-packages (from sdv) (14.2.1)\n",
            "Requirement already satisfied: graphviz<1,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.20.1)\n",
            "Requirement already satisfied: tqdm<5,>=4.15 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.65.0)\n",
            "Requirement already satisfied: copulas<0.10,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.9.0)\n",
            "Requirement already satisfied: ctgan<0.8,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.7.4)\n",
            "Requirement already satisfied: deepecho<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.4.2)\n",
            "Requirement already satisfied: rdt<2,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.6.1)\n",
            "Requirement already satisfied: sdmetrics<0.11,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.10.1)\n",
            "Requirement already satisfied: cloudpickle<3.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.2.1)\n",
            "Requirement already satisfied: boto3<2,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.28.23)\n",
            "Requirement already satisfied: botocore<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.31.23)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.5.3)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.15.0->sdv) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.15.0->sdv) (0.6.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<2,>=1.18->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<2,>=1.18->sdv) (1.26.16)\n",
            "Requirement already satisfied: matplotlib<4,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from copulas<0.10,>=0.9.0->sdv) (3.7.1)\n",
            "Requirement already satisfied: scipy<2,>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from copulas<0.10,>=0.9.0->sdv) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from ctgan<0.8,>=0.7.2->sdv) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from ctgan<0.8,>=0.7.2->sdv) (1.13.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->sdv) (2022.7.1)\n",
            "Requirement already satisfied: psutil<6,>=5.7 in /usr/local/lib/python3.10/dist-packages (from rdt<2,>=1.5.0->sdv) (5.9.5)\n",
            "Requirement already satisfied: plotly<6,>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from sdmetrics<0.11,>=0.10.0->sdv) (5.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6,>=5.10.0->sdmetrics<0.11,>=0.10.0->sdv) (8.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2,>=1.18->sdv) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.2->sdv) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.2->sdv) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->ctgan<0.8,>=0.7.2->sdv) (0.41.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.41.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Presidio\n",
        "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "\n",
        "# For extracting text\n",
        "from pdfminer.high_level import extract_text, extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n",
        "\n",
        "#For grouping\n",
        "from operator import itemgetter\n",
        "from itertools import groupby\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#For Transform & Anonymize\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from rdt import HyperTransformer\n",
        "from rdt.transformers.pii import PseudoAnonymizedFaker\n",
        "\n",
        "\n",
        "import logging\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "import click\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "AutoModelForCausalLM,\n",
        "AutoTokenizer,\n",
        "DataCollatorForLanguageModeling,\n",
        "PreTrainedTokenizer,\n",
        "Trainer,\n",
        "TrainingArguments,\n",
        "set_seed,\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n"
      ],
      "metadata": {
        "id": "S-5SyBjml8h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze the text in the PDF**"
      ],
      "metadata": {
        "id": "M7ye8u34mH3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "analyzed_character_sets = []\n",
        "characters_final = []\n",
        "characters_2 = []\n",
        "start_lst = []\n",
        "end_lst = []\n",
        "\n",
        "for page_layout in extract_pages(\"./PII_Sample.pdf\"):\n",
        "    for text_container in page_layout:\n",
        "        if isinstance(text_container, LTTextContainer):\n",
        "\n",
        "            # The element is a LTTextContainer, containing a paragraph of text.\n",
        "            text_to_anonymize = text_container.get_text()\n",
        "\n",
        "            # Analyze the text using the analyzer engine\n",
        "            analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n",
        "\n",
        "\n",
        "            characters = list([])\n",
        "\n",
        "            # Grab the characters from the PDF\n",
        "            for text_line in text_to_anonymize:\n",
        "                    characters.append(text_line)\n",
        "\n",
        "            # Slice out the characters that match the analyzer results.\n",
        "            for i in range(0, len(analyzer_results)):\n",
        "                start = analyzer_results[i].start\n",
        "                end = analyzer_results[i].end\n",
        "                analyzed_character_sets.append({\"characters\": ''.join(characters[start:end]), \"entity_type\": analyzer_results[i].entity_type})\n",
        "                characters_2.append(characters)\n",
        "                start_lst.append(start)\n",
        "                end_lst.append(end)\n",
        "            characters_final.append(characters)\n",
        "df = pd.DataFrame.from_records(analyzed_character_sets).groupby(['entity_type'])['characters'].apply(list)\n",
        "new_person = []\n",
        "for x in df.PERSON:\n",
        "  if '\\n' in x:\n",
        "    for y in x.split('\\n'):\n",
        "      new_person.append(y)\n",
        "  else:\n",
        "    new_person.append(x)\n",
        "df.PERSON = new_person\n",
        "data = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in df.items() ]))\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "AlVNAD0xmJhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f63a35-b768-4ea8-aaaf-62e2ef25d682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:configuration file /usr/local/lib/python3.10/dist-packages/conf/default.yaml not found.  Using default config: {'nlp_engine_name': 'spacy', 'models': [{'lang_code': 'en', 'model_name': 'en_core_web_lg'}]}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DATE_TIME      LOCATION MEDICAL_LICENSE           PERSON  \\\n",
            "0  1982-02-22      TN 48511       CV6642133      James Clark   \n",
            "1  1971-04-20  West Gabriel       EX4578247  Kathryn Alvarez   \n",
            "2  2018-11-19           NaN             NaN  Nathaniel Smith   \n",
            "3  2007-06-17           NaN             NaN      North Aaron   \n",
            "4         NaN           NaN             NaN    Billy Parrish   \n",
            "\n",
            "        PHONE_NUMBER  \n",
            "0       805-454-3206  \n",
            "1       668.951.1735  \n",
            "2  971.439.0527x7186  \n",
            "3                NaN  \n",
            "4                NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform & Anonymize Data**"
      ],
      "metadata": {
        "id": "DWQH435h6cxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=data)\n",
        "metadata.update_column(\n",
        "    column_name='DATE_TIME',\n",
        "    sdtype='datetime',\n",
        "    datetime_format='%Y-%m-%d')\n",
        "metadata.update_column(\n",
        "    column_name='MEDICAL_LICENSE',\n",
        "    sdtype='bban')\n",
        "metadata.update_column(\n",
        "    column_name='PERSON',\n",
        "    sdtype='name')\n",
        "metadata.update_column(\n",
        "    column_name='PHONE_NUMBER',\n",
        "    sdtype='phone_number')\n",
        "synthesizer = HyperTransformer()\n",
        "synthesizer.detect_initial_config(data)\n",
        "synthesizer.update_sdtypes(column_name_to_sdtype={\n",
        "  'DATE_TIME': 'datetime',\n",
        "  'MEDICAL_LICENSE': 'pii',\n",
        "  'PERSON': 'pii',\n",
        "  'PHONE_NUMBER': 'pii'\n",
        "})\n",
        "synthesizer.update_transformers(column_name_to_transformer={\n",
        "    'MEDICAL_LICENSE': PseudoAnonymizedFaker(provider_name='bank', function_name='bban'),\n",
        "    'PERSON': PseudoAnonymizedFaker(provider_name='person', function_name='name'),\n",
        "    'PHONE_NUMBER': PseudoAnonymizedFaker(provider_name='phone_number', function_name='phone_number'),\n",
        "})\n",
        "synthesizer.fit(data)\n",
        "med_license = synthesizer.get_config()['transformers']['MEDICAL_LICENSE']\n",
        "paf = synthesizer.get_config()['transformers']['PERSON']\n",
        "phone_number = synthesizer.get_config()['transformers']['PHONE_NUMBER']\n",
        "\n",
        "pii_k = []\n",
        "pii_v = []\n",
        "for k, v in med_license.get_mapping().items():\n",
        "    pii_k.append(k)\n",
        "    pii_v.append(v)\n",
        "for k, v in paf.get_mapping().items():\n",
        "    pii_k.append(k)\n",
        "    pii_v.append(v)\n",
        "for k, v in phone_number.get_mapping().items():\n",
        "    pii_k.append(k)\n",
        "    pii_v.append(v)\n",
        "\n",
        "pii_k = np.array(pii_k)\n",
        "pii_v = np.array(pii_v)\n",
        "\n",
        "characters_final = ''.join([item for sublist in characters_final for item in sublist])\n",
        "for i in range(0, len(characters_2)):\n",
        "  pii_1 = ''.join(characters_2[i][start_lst[i]:end_lst[i]])\n",
        "  if pii_1 in characters_final:\n",
        "    value_1 = pii_v[np.argwhere(pii_1==pii_k).flatten()]\n",
        "    if value_1.size > 0:\n",
        "      characters_final = characters_final.replace(pii_1, value_1[0])\n",
        "\n",
        "#Final Anonymized string which hides PII\n",
        "print(characters_final)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxwl87DP6hpx",
        "outputId": "642c8a04-e787-4eb8-f3b0-c33fb0017f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Dear Madam/Sir,\n",
            "Andrea Lawrence\n",
            "13002 Roberts Mountain Apt. 930\n",
            "Beckshire, GU 38939\n",
            "Phone No.: (054)080-8788\n",
            "Can you please provide an update for the debit card for the checking account number\n",
            "IVTXKT500282463110248943623?\n",
            "Thanks,\n",
            "Dated: 1982-02-22\n",
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Hello,\n",
            "Colleen Williams\n",
            "31253 Smith Bypass Suite 355\n",
            "Rogersfurt, IA 59691\n",
            "Phone No.: 745.786.3221x2401\n",
            "Phone\n",
            "Where is the card I ordered for my bank account number EINOGH868803981530131508389?\n",
            "Best,\n",
            "Dated: 1971-04-20\n",
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Good Morning,\n",
            "Debbie Byrd\n",
            "5271 Bolton Cliffs Suite 617\n",
            "Bryan Harris, TN 48511\n",
            "Phone No.: +1-539-158-3596x328\n",
            "Is it confirmed if I have a new card coming in the mail for the checking account number\n",
            "QQQS27528544402592?\n",
            "Best,\n",
            "Dated: 2018-11-19\n",
            "ACCOUNT WITHDRAWAL REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Good Morning,\n",
            "Robert Hunt\n",
            "2516 Sara Mountains\n",
            "West Gabriel, NY 17877\n",
            "Phone No.: 411-904-6891\n",
            "Can you prepare a withdrawal request in the amount of $15,000 from my checking account\n",
            "number TTVY24567636706751?\n",
            "Best,\n",
            "Dated: 2007-06-17\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reverse Transformation of PII**"
      ],
      "metadata": {
        "id": "WKTj7GNu0Vrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters_original = characters_final\n",
        "for i in range(0, len(pii_v)):\n",
        "  characters_original = characters_original.replace(pii_v[i], pii_k[i])\n",
        "print(characters_original)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfVt8HuW0d1E",
        "outputId": "ac20a0da-0930-4928-db8d-368bb21f4840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Dear Madam/Sir,\n",
            "James Clark\n",
            "13002 Roberts Mountain Apt. 930\n",
            "Beckshire, GU 38939\n",
            "Phone No.: 805-454-3206\n",
            "Can you please provide an update for the debit card for the checking account number\n",
            "IVCV66421338943623?\n",
            "Thanks,\n",
            "Dated: 1982-02-22\n",
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Hello,\n",
            "Kathryn Alvarez\n",
            "31253 Smith Bypass Suite 355\n",
            "Rogersfurt, IA 59691\n",
            "Phone No.: 668.951.1735\n",
            "Phone\n",
            "Where is the card I ordered for my bank account number EIEX45782471508389?\n",
            "Best,\n",
            "Dated: 1971-04-20\n",
            "DEBIT CARD REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Good Morning,\n",
            "Nathaniel Smith\n",
            "5271 Bolton Cliffs Suite 617\n",
            "North Aaron, TN 48511\n",
            "Phone No.: 971.439.0527x7186\n",
            "Is it confirmed if I have a new card coming in the mail for the checking account number\n",
            "QQQS27528544402592?\n",
            "Best,\n",
            "Dated: 2018-11-19\n",
            "ACCOUNT WITHDRAWAL REQUEST\n",
            "To:\n",
            "The Branch Manager\n",
            "Bank of America\n",
            "Good Morning,\n",
            "Billy Parrish\n",
            "2516 Sara Mountains\n",
            "West Gabriel, NY 17877\n",
            "Phone No.: 411-904-6891\n",
            "Can you prepare a withdrawal request in the amount of $15,000 from my checking account\n",
            "number TTVY24567636706751?\n",
            "Best,\n",
            "Dated: 2007-06-17\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract questions from anonymized string for use in LLM**"
      ],
      "metadata": {
        "id": "dndAvrOq0I95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = characters_final\n",
        "\n",
        "questions = []\n",
        "while True:\n",
        "  sub1 = 'No.:'\n",
        "  sub2 = '?'\n",
        "\n",
        "  idx1 = words.find(sub1)\n",
        "  if idx1 == -1:\n",
        "    break\n",
        "  idx2 = words.find(sub2)\n",
        "\n",
        "  for i in range(0, len(words)):\n",
        "    res = ''\n",
        "    for idx in range(idx1 + len(sub1) + 1, idx2):\n",
        "        res = res + words[idx]\n",
        "  questions.append(res)\n",
        "  words = words[idx2+1:]\n",
        "\n",
        "questions = np.array(questions)\n",
        "labels = np.array([11,11,11,46])\n",
        "\n",
        "fine_tuning_training_set = pd.DataFrame({'text':questions, 'label':labels})\n",
        "print(fine_tuning_training_set)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HX51VGNzsTI",
        "outputId": "f4a51204-73fc-4569-cd5e-105e426b424f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  (054)080-8788\\nCan you please provide an updat...     11\n",
            "1  745.786.3221x2401\\nPhone\\nWhere is the card I ...     11\n",
            "2  +1-539-158-3596x328\\nIs it confirmed if I have...     11\n",
            "3  411-904-6891\\nCan you prepare a withdrawal req...     46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM Model**"
      ],
      "metadata": {
        "id": "8qmzrzB-TT6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training_dataset(path_or_dataset: str = \"PolyAI/banking77\") -> Dataset:\n",
        "  logger.info(f\"Loading dataset from {path_or_dataset}\")\n",
        "  dataset = load_dataset(path_or_dataset)[\"train\"]\n",
        "  logger.info(\"Found %d rows\", dataset.num_rows)\n",
        "def _add_text(rec):\n",
        "  instruction = rec[\"instruction\"]\n",
        "  response = rec[\"response\"]\n",
        "  context = rec.get(\"context\")\n",
        "  if not instruction:\n",
        "    raise ValueError(f\"Expected an instruction in: {rec}\")\n",
        "  if not response:\n",
        "    raise ValueError(f\"Expected a response in: {rec}\")\n",
        "  if context:\n",
        "    rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
        "  else:\n",
        "    rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
        "    return rec\n",
        "  dataset = dataset.map(_add_text)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "0V7_GpVFThbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed=42) -> Dataset:\n",
        "  dataset = load_training_dataset()\n",
        "  print('dataset')\n",
        "  print(dataset[0])\n",
        "  logger.info(\"Preprocessing dataset\")\n",
        "  _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "  dataset = dataset.map(\n",
        "  _preprocessing_function,\n",
        "  batched=True,\n",
        "  remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
        "  )\n",
        "  logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n",
        "  dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
        "  logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
        "  logger.info(\"Shuffling dataset\")\n",
        "  dataset = dataset.shuffle(seed=seed)\n",
        "  logger.info(\"Done preprocessing\")\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "LZr5BwWtTy0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  *,\n",
        "  input_model: str,\n",
        "  local_output_dir: str,\n",
        "  dbfs_output_dir: str,\n",
        "  epochs: int,\n",
        "  per_device_train_batch_size: int,\n",
        "  per_device_eval_batch_size: int,\n",
        "  lr: float,\n",
        "  seed: int,\n",
        "  deepspeed: str,\n",
        "  gradient_checkpointing: bool,\n",
        "  local_rank: str,\n",
        "  bf16: bool,\n",
        "  logging_steps: int,\n",
        "  save_steps: int,\n",
        "  eval_steps: int,\n",
        "  test_size: Union[float, int],\n",
        "  save_total_limit: int,\n",
        "  warmup_steps: int,\n",
        "  ):\n",
        "  set_seed(seed)\n",
        "\n",
        "  model, tokenizer = get_model_tokenizer(\n",
        "  pretrained_model_name_or_path=input_model, gradient_checkpointing=gradient_checkpointing\n",
        "  )\n",
        "\n",
        "  # Use the same max length that the model supports. Fall back to 1024 if the setting can't be found.\n",
        "  # The configuraton for the length can be stored under different names depending on the model. Here we attempt\n",
        "  # a few possible names we've encountered.\n",
        "  conf = model.config\n",
        "  max_length = None\n",
        "  for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "    max_length = getattr(model.config, length_setting, None)\n",
        "    if max_length:\n",
        "      logger.info(f\"Found max lenth: {max_length}\")\n",
        "      break\n",
        "    if not max_length:\n",
        "      max_length = 1024\n",
        "      logger.info(f\"Using default max length: {max_length}\")\n",
        "\n",
        "  processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length, seed=seed)\n",
        "\n",
        "  split_dataset = processed_dataset.train_test_split(test_size=test_size, seed=seed)\n",
        "\n",
        "  logger.info(\"Train data size: %d\", split_dataset[\"train\"].num_rows)\n",
        "  logger.info(\"Test data size: %d\", split_dataset[\"test\"].num_rows)\n",
        "\n",
        "  data_collator = DataCollatorForCompletionOnlyLM(\n",
        "  tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
        "  )\n",
        "\n",
        "  if not dbfs_output_dir:\n",
        "    logger.warn(\"Will NOT save to DBFS\")\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "  output_dir=local_output_dir,\n",
        "  per_device_train_batch_size=per_device_train_batch_size,\n",
        "  per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "  fp16=False,\n",
        "  bf16=bf16,\n",
        "  learning_rate=lr,\n",
        "  num_train_epochs=epochs,\n",
        "  deepspeed=deepspeed,\n",
        "  gradient_checkpointing=gradient_checkpointing,\n",
        "  logging_dir=f\"{local_output_dir}/runs\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=logging_steps,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  eval_steps=eval_steps,\n",
        "  save_strategy=\"steps\",\n",
        "  save_steps=save_steps,\n",
        "  save_total_limit=save_total_limit,\n",
        "  load_best_model_at_end=False,\n",
        "  report_to=\"tensorboard\",\n",
        "  disable_tqdm=True,\n",
        "  remove_unused_columns=False,\n",
        "  local_rank=local_rank,\n",
        "  warmup_steps=warmup_steps,\n",
        "  )\n",
        "\n",
        "  logger.info(\"Instantiating Trainer\")\n",
        "\n",
        "  trainer = Trainer(\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  args=training_args,\n",
        "  train_dataset=split_dataset[\"train\"],\n",
        "  eval_dataset=split_dataset[\"test\"],\n",
        "  data_collator=data_collator,\n",
        "  )\n",
        "\n",
        "  logger.info(\"Training\")\n",
        "  trainer.train()\n",
        "\n",
        "  logger.info(f\"Saving Model to {local_output_dir}\")\n",
        "  trainer.save_model(output_dir=local_output_dir)\n",
        "\n",
        "  if dbfs_output_dir:\n",
        "    logger.info(f\"Saving Model to {dbfs_output_dir}\")\n",
        "  trainer.save_model(output_dir=dbfs_output_dir)\n",
        "\n",
        "  logger.info(\"Done.\")"
      ],
      "metadata": {
        "id": "XBtrrHBVT3IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# load pre-trained language model and tokenizer\n",
        "model_name = \"microsoft/CodeGPT-small-java\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# prepare data for fine-tuning\n",
        "train_texts = fine_tuning_training_set.values[:, 0].tolist()\n",
        "train_labels = fine_tuning_training_set.values[:, 1].tolist()\n",
        "\n",
        "print(train_texts)\n",
        "print(train_labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "\n",
        "class FTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['label'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = FTDataset(train_encodings, train_labels)\n",
        "print(train_dataset[0])\n",
        "\n",
        "# fine-tune the model\n",
        "training_args = TrainingArguments(\n",
        "output_dir='./results',\n",
        "evaluation_strategy = \"epoch\",\n",
        "learning_rate=.00001,\n",
        "per_device_train_batch_size=1,\n",
        "per_device_eval_batch_size=1,\n",
        "num_train_epochs=1,\n",
        "weight_decay=0.01,\n",
        "push_to_hub=False,\n",
        ")\n",
        "trainer = Trainer(\n",
        "model=model,\n",
        "args=training_args,\n",
        "train_dataset=train_dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jik5si47T3yf",
        "outputId": "74090add-a623-446b-e3c4-16ef068f25c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(054)080-8788\\nCan you please provide an update for the debit card for the checking account number\\nIVTXKT500282463110248943623', '745.786.3221x2401\\nPhone\\nWhere is the card I ordered for my bank account number EINOGH868803981530131508389', '+1-539-158-3596x328\\nIs it confirmed if I have a new card coming in the mail for the checking account number\\nQQQS27528544402592', '411-904-6891\\nCan you prepare a withdrawal request in the amount of $15,000 from my checking account\\nnumber TTVY24567636706751']\n",
            "[11, 11, 11, 46]\n",
            "{'input_ids': tensor([   10,    18,  6065,    11,    18,  2679,    15,  5939,  4760,   201,\n",
            "         2203,  5749, 16276,  8959,   916,  1400,   438,   463,   578,  4449,\n",
            "         8348,   438,   463, 10801,  4198,  2276,   201,  5697,  9993, 36500,\n",
            "         7943,  4422,  2936, 12151, 22031,  2936,  5775,    22,  5629,  3987,\n",
            "            1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]), 'label': tensor(11)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-a51b89bfa556>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (44) to match target batch_size (0)."
          ]
        }
      ]
    }
  ]
}